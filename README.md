# SETC (A Standard Evaluation Tool for Chatbots)
Chatbot evaluation is really hard. There is no standard, and this is our attempt to at least address small parts of this issue.

Right now we are attempting to use Parl AI as our framework http://parl.ai/. 

All of our checkpoints will be made publicly available including all configurations. See this [link](http://chatbot-eval-data.s3-accelerate.amazonaws.com/results/available_checkpoints.txt) for checkpoints from the paper. We use [OpenMNT-py](https://github.com/OpenNMT/OpenNMT-py) for our experiments.

Submit your model! Please take a look https://goo.gl/forms/26Wu13jYBpxWoAgx1

Amazon Mechanical Turk is not free... we are actively looking for funding.

Please find our paper [here](Chatbot_Evaluation_2018_COLING.pdf).

**WARNING: This is extremely alpha code!!!**


