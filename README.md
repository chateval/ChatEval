# [SETC (A Standard Evaluation Tool for Chatbots)](https://jsedoc.github.io/SETC/)
Chatbot evaluation is really hard. There is [no standard](http://www.seas.upenn.edu/~joao/chatbot_human_evaluation.pdf), and this is our attempt to at least address small parts of this issue.

Right now we using [ParlAi](http://parl.ai/) as our framework for data as well as experiments. We use [OpenMNT-py](https://github.com/OpenNMT/OpenNMT-py) for training models. All of our checkpoints will be made publicly available including all configurations. See this [link](http://chatbot-eval-data.s3-accelerate.amazonaws.com/results/available_checkpoints.txt) for checkpoints from the paper. 

Submit your model! Please take a look https://goo.gl/forms/26Wu13jYBpxWoAgx1

Amazon Mechanical Turk is not free... we are actively looking for funding.

Please find our paper [here](Chatbot_Evaluation_2018_COLING.pdf).

**WARNING: This is extremely alpha code!!!**


