# [ChatEval (A Tool for Evaluating Chatbots)](https://chateval.org/)
Chatbot evaluation is really hard. There is [no standard](http://www.seas.upenn.edu/~joao/chatbot_human_evaluation.pdf), and this is our attempt to at least address small parts of this issue.

Right now we using [ParlAi](http://parl.ai/) as our framework for data as well as experiments. We used [OpenMNT-py](https://github.com/OpenNMT/OpenNMT-py) for training models. All of our checkpoints will be made publicly available including all configurations. See this [link](http://chatbot-eval-data.s3.amazonaws.com/results/available_checkpoints.txt) for checkpoints from the paper. 

Submit your model! Please take a look our submission [page](https://my.chateval.org/).

Amazon Mechanical Turk is not free... we have received initial funding from Joao Sedoc's MSR Dissertation Grant. Thank you Microsoft!

Please find our paper [here](http://cis.upenn.edu/~ccb/publications/chateval.pdf).

**What does ChatEval solve?**
 1. Shared and publicly available model code and checkpoints.
 1. Standard evaluation datasets.
 1. Standard human annotator framework (currently using Amazon Mechanical Turk).
 1. Model comparisons of the performance of Model A vs Model B. Both a summary and all data are available.

